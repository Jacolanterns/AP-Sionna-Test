{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00c188d",
   "metadata": {},
   "source": [
    "# Data Acquisition Process for WiFi Signal Strength Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the data acquisition process for collecting and processing WiFi signal strength data from access points (APs) in a building. The process involves:\n",
    "\n",
    "1. **Environment Setup**: Import libraries and configure parameters\n",
    "2. **Data Loading**: Read data from JSON/CSV files or InfluxDB\n",
    "3. **Data Processing**: Filter 2.4GHz band, clean data, and process BSSID information\n",
    "4. **AP Integration**: Merge AP location data from coordinate files\n",
    "5. **Data Export**: Save processed data to CSV format\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "- **InfluxDB**: Database option (configurable but not currently used)\n",
    "- **JSON File**: Primary data source (aruba_07_15.json) with WiFi measurements\n",
    "- **AP Coordinate Files**: CSV files (3f.csv, 2f.csv) with AP location data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050c19c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import necessary libraries and load environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# InfluxDB imports (optional - install with: pip install influxdb-client)\n",
    "try:\n",
    "    from influxdb_client import InfluxDBClient, Point, WriteOptions\n",
    "    from influxdb_client.client.query_api import QueryOptions\n",
    "    from influxdb_client.client.warnings import MissingPivotFunction\n",
    "    warnings.simplefilter(\"ignore\", MissingPivotFunction)\n",
    "    INFLUXDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"InfluxDB client not available. Install with: pip install influxdb-client\")\n",
    "    INFLUXDB_AVAILABLE = False\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration parameters\n",
    "USE_INFLUXDB = True  # Set to True to use InfluxDB, False to use file\n",
    "INFLUXDB_CREDENTIALS_PATH = \"credentials.yml\"  # Path to InfluxDB credentials\n",
    "\n",
    "# Available data files found in the system\n",
    "DATA_SOURCE_CSV = \"/home/sionna/Documents/ghulam/rssi_d1_2f.csv\"  # RSSI data for 2F\n",
    "DATA_SOURCE_JSON = \"/home/sionna/Documents/GitHub/nvidia-sionna/AP BSS Table.json\"  # AP BSS table\n",
    "ALTERNATIVE_DATA_FILES = {\n",
    "    \"full_rssi_d1\": \"/home/sionna/Documents/ghulam/full_rssi_d1.csv\",\n",
    "    \"nvidia_rssi_2f\": \"/home/sionna/Documents/GitHub/nvidia-sionna/full_rssi_d1_2f.csv\",\n",
    "    \"mapped_rssi\": \"/home/sionna/Documents/GitHub/nvidia-sionna/mapped_rssi_d1_2f.csv\"\n",
    "}\n",
    "\n",
    "# Since we don't have coordinate files, we'll create sample ones or extract from AP names\n",
    "AP_COORD_FILES = {\n",
    "    \"2f\": \"2f.csv\",  # Will be created if not found\n",
    "    \"3f\": \"3f.csv\"   # Will be created if not found\n",
    "}\n",
    "OUTPUT_FILE = \"ap_data.csv\"\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"InfluxDB available: {INFLUXDB_AVAILABLE}\")\n",
    "print(f\"Use InfluxDB: {USE_INFLUXDB}\")\n",
    "print(f\"InfluxDB credentials: {os.path.exists(INFLUXDB_CREDENTIALS_PATH) if USE_INFLUXDB else 'N/A'}\")\n",
    "print(f\"Primary RSSI data source: {DATA_SOURCE_CSV}\")\n",
    "print(f\"AP BSS table: {DATA_SOURCE_JSON}\")\n",
    "print(f\"Alternative data files available: {len(ALTERNATIVE_DATA_FILES)}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "\n",
    "# Check which files actually exist\n",
    "print(f\"\\nFile availability check:\")\n",
    "if USE_INFLUXDB and INFLUXDB_AVAILABLE:\n",
    "    print(f\"  InfluxDB credentials exist: {os.path.exists(INFLUXDB_CREDENTIALS_PATH)}\")\n",
    "print(f\"  RSSI CSV exists: {os.path.exists(DATA_SOURCE_CSV)}\")\n",
    "print(f\"  AP BSS JSON exists: {os.path.exists(DATA_SOURCE_JSON)}\")\n",
    "for name, path in ALTERNATIVE_DATA_FILES.items():\n",
    "    print(f\"  {name}: {os.path.exists(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb7b97",
   "metadata": {},
   "source": [
    "## 2. Load Data from File or InfluxDB\n",
    "\n",
    "Load WiFi signal strength data from the configured source (JSON file or InfluxDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac64a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(file_path):\n",
    "    \"\"\"Load data from JSON or CSV file\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.json'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"Successfully loaded JSON data from {file_path}\")\n",
    "            return data\n",
    "        elif file_path.endswith('.csv'):\n",
    "            # For large CSV files, read in chunks to avoid memory issues\n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                print(f\"Successfully loaded CSV data from {file_path}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading large CSV, trying chunked reading: {str(e)}\")\n",
    "                # Read first 10000 rows for testing\n",
    "                data = pd.read_csv(file_path, nrows=10000)\n",
    "                print(f\"Loaded first 10000 rows from {file_path}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                return data\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use JSON or CSV.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_influxdb():\n",
    "    \"\"\"Load data from InfluxDB using credentials\"\"\"\n",
    "    if not INFLUXDB_AVAILABLE:\n",
    "        print(\"InfluxDB client not available. Install with: pip install influxdb-client\")\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(INFLUXDB_CREDENTIALS_PATH):\n",
    "        print(f\"InfluxDB credentials file not found: {INFLUXDB_CREDENTIALS_PATH}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load credentials\n",
    "        with open(INFLUXDB_CREDENTIALS_PATH, 'r') as stream:\n",
    "            credentials = yaml.safe_load(stream)['influxdb']\n",
    "        \n",
    "        print(f\"Connecting to InfluxDB at: {credentials['url']}\")\n",
    "        \n",
    "        # Create client\n",
    "        client = InfluxDBClient(\n",
    "            url=credentials['url'], \n",
    "            token=credentials['token'], \n",
    "            org=credentials['org'], \n",
    "            timeout=60000\n",
    "        )\n",
    "        \n",
    "        # Define query for AP data\n",
    "        query = '''from(bucket: \"aruba\")  \n",
    "            |> range(start: -24h, stop: now())\n",
    "            |> filter(fn: (r) => r[\"_measurement\"] == \"adjacent-ap\")\n",
    "            |> filter(fn: (r) => r[\"ap_type\"] == \"valid\")\n",
    "            |> filter(fn: (r) => r[\"_field\"] == \"building\" or\n",
    "                                 r[\"_field\"] == \"floor\" or\n",
    "                                 r[\"_field\"] == \"monitoring-ap\" or\n",
    "                                 r[\"_field\"] == \"curr-rssi\" or\n",
    "                                 r[\"_field\"] == \"essid\")\n",
    "            |> group()'''\n",
    "        \n",
    "        print(\"Executing InfluxDB query...\")\n",
    "        query_api = client.query_api()\n",
    "        data_frame = query_api.query_data_frame(org=credentials['org'], query=query)\n",
    "        \n",
    "        print(f\"Retrieved {len(data_frame)} records from InfluxDB\")\n",
    "        return data_frame\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to InfluxDB: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load data based on configuration\n",
    "if USE_INFLUXDB:\n",
    "    raw_data = load_data_from_influxdb()\n",
    "else:\n",
    "    # Try to load the RSSI CSV file first\n",
    "    print(\"Attempting to load RSSI data...\")\n",
    "    raw_data = load_data_from_file(DATA_SOURCE_CSV)\n",
    "    \n",
    "    if raw_data is None:\n",
    "        print(\"Primary file not available, trying alternative files...\")\n",
    "        for name, path in ALTERNATIVE_DATA_FILES.items():\n",
    "            if os.path.exists(path):\n",
    "                print(f\"Trying {name}: {path}\")\n",
    "                raw_data = load_data_from_file(path)\n",
    "                if raw_data is not None:\n",
    "                    DATA_SOURCE_CSV = path  # Update the source path\n",
    "                    break\n",
    "\n",
    "if raw_data is not None:\n",
    "    print(f\"\\nData loaded successfully. Type: {type(raw_data)}\")\n",
    "    if isinstance(raw_data, pd.DataFrame):\n",
    "        print(f\"DataFrame shape: {raw_data.shape}\")\n",
    "        print(f\"Columns: {list(raw_data.columns)}\")\n",
    "    elif isinstance(raw_data, list):\n",
    "        print(f\"Number of records: {len(raw_data)}\")\n",
    "    elif isinstance(raw_data, dict):\n",
    "        print(f\"Data keys: {list(raw_data.keys())}\")\n",
    "else:\n",
    "    print(\"Failed to load data. Please check the file paths and formats.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccf91d",
   "metadata": {},
   "source": [
    "## 3. Convert Data to DataFrame\n",
    "\n",
    "Convert the loaded data into a pandas DataFrame for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(data):\n",
    "    \"\"\"Convert raw data to pandas DataFrame\"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if isinstance(data, list):\n",
    "            # Assuming data is a list of dictionaries\n",
    "            df = pd.DataFrame(data)\n",
    "        elif isinstance(data, dict):\n",
    "            # Handle different JSON structures\n",
    "            if 'data' in data:\n",
    "                df = pd.DataFrame(data['data'])\n",
    "            elif 'results' in data:\n",
    "                df = pd.DataFrame(data['results'])\n",
    "            else:\n",
    "                # Try to flatten the dictionary\n",
    "                df = pd.json_normalize(data)\n",
    "        else:\n",
    "            # Data is already a DataFrame\n",
    "            df = data\n",
    "        \n",
    "        print(f\"DataFrame created successfully!\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting data to DataFrame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Convert raw data to DataFrame\n",
    "df = convert_to_dataframe(raw_data)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Failed to create DataFrame from loaded data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb6e4f",
   "metadata": {},
   "source": [
    "## 4. Filter and Clean Data\n",
    "\n",
    "Filter data to include only 2.4GHz band, remove NaN values, and keep only APs from floors 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6690aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_clean_data(df):\n",
    "    \"\"\"Filter and clean the WiFi data\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "        \n",
    "        # Assuming common column names - adjust based on actual data structure\n",
    "        # These are typical column names for WiFi data\n",
    "        possible_freq_cols = ['frequency', 'freq', 'band', 'channel']\n",
    "        possible_rssi_cols = ['rssi', 'signal_strength', 'power']\n",
    "        possible_bssid_cols = ['bssid', 'mac', 'ap_mac']\n",
    "        possible_ap_cols = ['ap_name', 'ap', 'access_point']\n",
    "        \n",
    "        freq_col = None\n",
    "        rssi_col = None\n",
    "        bssid_col = None\n",
    "        ap_col = None\n",
    "        \n",
    "        # Find the actual column names in the DataFrame\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(freq in col_lower for freq in possible_freq_cols):\n",
    "                freq_col = col\n",
    "            if any(rssi in col_lower for rssi in possible_rssi_cols):\n",
    "                rssi_col = col\n",
    "            if any(bssid in col_lower for bssid in possible_bssid_cols):\n",
    "                bssid_col = col\n",
    "            if any(ap in col_lower for ap in possible_ap_cols):\n",
    "                ap_col = col\n",
    "        \n",
    "        print(f\"Detected columns:\")\n",
    "        print(f\"  Frequency: {freq_col}\")\n",
    "        print(f\"  RSSI: {rssi_col}\")\n",
    "        print(f\"  BSSID: {bssid_col}\")\n",
    "        print(f\"  AP Name: {ap_col}\")\n",
    "        \n",
    "        # Filter for 2.4GHz band (typically 2400-2500 MHz)\n",
    "        if freq_col:\n",
    "            df_filtered = df[df[freq_col].between(2400, 2500)].copy()\n",
    "            print(f\"After 2.4GHz filtering: {df_filtered.shape}\")\n",
    "        else:\n",
    "            print(\"No frequency column found, skipping frequency filtering\")\n",
    "            df_filtered = df.copy()\n",
    "        \n",
    "        # Remove rows with NaN RSSI values\n",
    "        if rssi_col:\n",
    "            df_filtered = df_filtered.dropna(subset=[rssi_col])\n",
    "            print(f\"After removing NaN RSSI: {df_filtered.shape}\")\n",
    "        \n",
    "        # Filter for floors 2 and 3 (look for patterns in AP names or BSSID)\n",
    "        if ap_col:\n",
    "            # Filter for APs containing 2F or 3F (floor indicators)\n",
    "            floor_pattern = df_filtered[ap_col].str.contains('2F|3F', case=False, na=False)\n",
    "            df_filtered = df_filtered[floor_pattern]\n",
    "            print(f\"After floor filtering (2F/3F): {df_filtered.shape}\")\n",
    "        elif bssid_col:\n",
    "            # Alternative: filter based on BSSID patterns if available\n",
    "            print(\"No AP name column found, using all data\")\n",
    "        \n",
    "        print(f\"Final filtered data shape: {df_filtered.shape}\")\n",
    "        return df_filtered\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering data: {str(e)}\")\n",
    "        return df\n",
    "\n",
    "# Apply filtering and cleaning\n",
    "if df is not None:\n",
    "    df_filtered = filter_and_clean_data(df)\n",
    "    \n",
    "    if df_filtered is not None and not df_filtered.empty:\n",
    "        print(f\"\\nFiltered data summary:\")\n",
    "        print(f\"Shape: {df_filtered.shape}\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        display(df_filtered.head())\n",
    "    else:\n",
    "        print(\"No data remaining after filtering, or filtering failed.\")\n",
    "        df_filtered = df  # Use original data if filtering fails\n",
    "else:\n",
    "    print(\"No DataFrame available for filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6680d23",
   "metadata": {},
   "source": [
    "## 5. Process BSSID and Pivot Data\n",
    "\n",
    "Match BSSID information with known APs and pivot data to wide format with RSSI columns for each AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bssid_and_pivot(df):\n",
    "    \"\"\"Process BSSID information and pivot data to wide format\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Find relevant columns\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        bssid_cols = [col for col in df.columns if any(x in col.lower() for x in ['bssid', 'mac', 'ap_mac'])]\n",
    "        rssi_cols = [col for col in df.columns if any(x in col.lower() for x in ['rssi', 'signal', 'power'])]\n",
    "        ap_cols = [col for col in df.columns if any(x in col.lower() for x in ['ap_name', 'ap', 'access_point'])]\n",
    "        essid_cols = [col for col in df.columns if any(x in col.lower() for x in ['essid', 'ssid', 'network'])]\n",
    "        \n",
    "        print(f\"Available columns for pivoting:\")\n",
    "        print(f\"  Time columns: {time_cols}\")\n",
    "        print(f\"  BSSID columns: {bssid_cols}\")\n",
    "        print(f\"  RSSI columns: {rssi_cols}\")\n",
    "        print(f\"  AP name columns: {ap_cols}\")\n",
    "        print(f\"  ESSID columns: {essid_cols}\")\n",
    "        \n",
    "        # Use the first available column of each type\n",
    "        time_col = time_cols[0] if time_cols else None\n",
    "        bssid_col = bssid_cols[0] if bssid_cols else None\n",
    "        rssi_col = rssi_cols[0] if rssi_cols else None\n",
    "        ap_col = ap_cols[0] if ap_cols else None\n",
    "        essid_col = essid_cols[0] if essid_cols else None\n",
    "        \n",
    "        if not (rssi_col and (bssid_col or ap_col)):\n",
    "            print(\"Missing required columns for pivoting (need RSSI and BSSID/AP name)\")\n",
    "            return df\n",
    "        \n",
    "        # Create AP identifier (prefer AP name over BSSID)\n",
    "        if ap_col:\n",
    "            df['ap_identifier'] = df[ap_col]\n",
    "        else:\n",
    "            df['ap_identifier'] = df[bssid_col]\n",
    "        \n",
    "        # Create unique AP names for RSSI columns\n",
    "        df['rssi_column'] = 'rssi_' + df['ap_identifier'].astype(str)\n",
    "        \n",
    "        # Pivot the data\n",
    "        pivot_cols = []\n",
    "        if time_col:\n",
    "            pivot_cols.append(time_col)\n",
    "        if essid_col:\n",
    "            pivot_cols.append(essid_col)\n",
    "        if bssid_col and ap_col:  # Keep original BSSID if we have both\n",
    "            pivot_cols.append(bssid_col)\n",
    "        \n",
    "        # Add any other non-pivoting columns\n",
    "        other_cols = [col for col in df.columns \n",
    "                     if col not in [rssi_col, ap_col if ap_col else bssid_col, 'ap_identifier', 'rssi_column']\n",
    "                     and col not in pivot_cols]\n",
    "        pivot_cols.extend(other_cols)\n",
    "        \n",
    "        if pivot_cols:\n",
    "            # Group by the pivot columns and create RSSI columns for each AP\n",
    "            df_pivot = df.pivot_table(\n",
    "                index=pivot_cols,\n",
    "                columns='rssi_column',\n",
    "                values=rssi_col,\n",
    "                aggfunc='mean'  # Average if multiple readings per time/AP\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            df_pivot.columns.name = None\n",
    "            \n",
    "            print(f\"Pivoted data shape: {df_pivot.shape}\")\n",
    "            print(f\"Number of RSSI columns created: {len([col for col in df_pivot.columns if col.startswith('rssi_')])}\")\n",
    "            \n",
    "            return df_pivot\n",
    "        else:\n",
    "            print(\"No suitable columns found for pivoting\")\n",
    "            return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in pivoting data: {str(e)}\")\n",
    "        print(\"Returning original data\")\n",
    "        return df\n",
    "\n",
    "# Process BSSID and pivot data\n",
    "if df_filtered is not None:\n",
    "    df_pivoted = process_bssid_and_pivot(df_filtered)\n",
    "    \n",
    "    if df_pivoted is not None:\n",
    "        print(f\"\\nPivoted data info:\")\n",
    "        print(f\"Shape: {df_pivoted.shape}\")\n",
    "        rssi_columns = [col for col in df_pivoted.columns if col.startswith('rssi_')]\n",
    "        print(f\"RSSI columns: {len(rssi_columns)}\")\n",
    "        if rssi_columns:\n",
    "            print(f\"Example RSSI columns: {rssi_columns[:5]}\")\n",
    "        \n",
    "        print(f\"\\nSample pivoted data:\")\n",
    "        display(df_pivoted.head())\n",
    "    else:\n",
    "        print(\"Pivoting failed, using filtered data\")\n",
    "        df_pivoted = df_filtered\n",
    "else:\n",
    "    print(\"No filtered data available for pivoting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e7b29",
   "metadata": {},
   "source": [
    "## 6. Load AP Coordinate Data\n",
    "\n",
    "Load AP location information from CSV files containing AP names and their X, Y, Z coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ac3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_ap_coordinates_from_bss_table():\n",
    "    \"\"\"Create sample AP coordinates from the BSS table if coordinate files don't exist\"\"\"\n",
    "    try:\n",
    "        # Load the AP BSS table to get AP names\n",
    "        if os.path.exists(DATA_SOURCE_JSON):\n",
    "            with open(DATA_SOURCE_JSON, 'r') as f:\n",
    "                bss_data = json.load(f)\n",
    "            \n",
    "            if \"Aruba AP BSS Table\" in bss_data:\n",
    "                ap_list = bss_data[\"Aruba AP BSS Table\"]\n",
    "                \n",
    "                # Extract unique AP names for floors 2 and 3\n",
    "                floor_2_aps = []\n",
    "                floor_3_aps = []\n",
    "                \n",
    "                for ap in ap_list:\n",
    "                    ap_name = ap.get(\"ap name\", \"\")\n",
    "                    if \"_2F_\" in ap_name:\n",
    "                        floor_2_aps.append(ap_name)\n",
    "                    elif \"_3F_\" in ap_name:\n",
    "                        floor_3_aps.append(ap_name)\n",
    "                \n",
    "                # Remove duplicates\n",
    "                floor_2_aps = list(set(floor_2_aps))\n",
    "                floor_3_aps = list(set(floor_3_aps))\n",
    "                \n",
    "                print(f\"Found {len(floor_2_aps)} unique APs for floor 2\")\n",
    "                print(f\"Found {len(floor_3_aps)} unique APs for floor 3\")\n",
    "                \n",
    "                # Create sample coordinate files\n",
    "                coordinates = {}\n",
    "                \n",
    "                if floor_2_aps:\n",
    "                    # Create sample coordinates for floor 2 (Z=6 meters)\n",
    "                    coords_2f = []\n",
    "                    for i, ap_name in enumerate(floor_2_aps):\n",
    "                        # Create a grid layout for demonstration\n",
    "                        x = (i % 10) * 10  # 10 meter spacing\n",
    "                        y = (i // 10) * 10\n",
    "                        z = 6.0  # 2nd floor height\n",
    "                        coords_2f.append([ap_name, x, y, z])\n",
    "                    \n",
    "                    df_2f = pd.DataFrame(coords_2f, columns=['AP_Name', 'X', 'Y', 'Z'])\n",
    "                    df_2f.to_csv('2f.csv', index=False)\n",
    "                    coordinates['2f'] = df_2f\n",
    "                    print(f\"Created 2f.csv with {len(df_2f)} APs\")\n",
    "                \n",
    "                if floor_3_aps:\n",
    "                    # Create sample coordinates for floor 3 (Z=9 meters)\n",
    "                    coords_3f = []\n",
    "                    for i, ap_name in enumerate(floor_3_aps):\n",
    "                        # Create a grid layout for demonstration\n",
    "                        x = (i % 10) * 10  # 10 meter spacing\n",
    "                        y = (i // 10) * 10\n",
    "                        z = 9.0  # 3rd floor height\n",
    "                        coords_3f.append([ap_name, x, y, z])\n",
    "                    \n",
    "                    df_3f = pd.DataFrame(coords_3f, columns=['AP_Name', 'X', 'Y', 'Z'])\n",
    "                    df_3f.to_csv('3f.csv', index=False)\n",
    "                    coordinates['3f'] = df_3f\n",
    "                    print(f\"Created 3f.csv with {len(df_3f)} APs\")\n",
    "                \n",
    "                return coordinates\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating coordinates from BSS table: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def load_ap_coordinates():\n",
    "    \"\"\"Load AP coordinate data from CSV files\"\"\"\n",
    "    ap_coords = {}\n",
    "    \n",
    "    # First try to load existing coordinate files\n",
    "    for floor, filename in AP_COORD_FILES.items():\n",
    "        try:\n",
    "            if os.path.exists(filename):\n",
    "                # Assuming CSV format: AP_Name,X_Coordinate,Y_Coordinate,Z_Coordinate\n",
    "                df_coords = pd.read_csv(filename)\n",
    "                print(f\"Loaded existing {filename}: {df_coords.shape}\")\n",
    "                \n",
    "                # Standardize column names\n",
    "                columns = df_coords.columns.tolist()\n",
    "                if len(columns) >= 4:\n",
    "                    df_coords.columns = ['AP_Name', 'X', 'Y', 'Z']\n",
    "                elif len(columns) == 3:\n",
    "                    df_coords.columns = ['AP_Name', 'X', 'Y']\n",
    "                    df_coords['Z'] = 0  # Default Z coordinate\n",
    "                \n",
    "                # Store coordinates for this floor\n",
    "                ap_coords[floor] = df_coords\n",
    "                print(f\"  Floor {floor}: {len(df_coords)} APs\")\n",
    "                print(f\"  Sample: {df_coords.head(2).to_string()}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"File {filename} not found for floor {floor}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "    \n",
    "    # If no coordinate files found, create sample ones\n",
    "    if not ap_coords:\n",
    "        print(\"No existing coordinate files found. Creating sample coordinates from BSS table...\")\n",
    "        ap_coords = create_sample_ap_coordinates_from_bss_table()\n",
    "    \n",
    "    # Combine all floor data\n",
    "    if ap_coords:\n",
    "        all_coords = pd.concat(ap_coords.values(), ignore_index=True)\n",
    "        print(f\"\\nCombined AP coordinates: {all_coords.shape}\")\n",
    "        return all_coords\n",
    "    else:\n",
    "        print(\"No AP coordinate files loaded successfully and couldn't create sample files\")\n",
    "        return None\n",
    "\n",
    "# Load AP coordinate data\n",
    "ap_coordinates = load_ap_coordinates()\n",
    "\n",
    "if ap_coordinates is not None:\n",
    "    print(f\"\\nAP Coordinates Summary:\")\n",
    "    print(f\"Total APs: {len(ap_coordinates)}\")\n",
    "    print(f\"Columns: {list(ap_coordinates.columns)}\")\n",
    "    print(f\"\\nSample coordinate data:\")\n",
    "    display(ap_coordinates.head())\n",
    "    \n",
    "    # Show coordinate ranges\n",
    "    if 'X' in ap_coordinates.columns and 'Y' in ap_coordinates.columns:\n",
    "        print(f\"\\nCoordinate ranges:\")\n",
    "        print(f\"X: {ap_coordinates['X'].min():.2f} to {ap_coordinates['X'].max():.2f}\")\n",
    "        print(f\"Y: {ap_coordinates['Y'].min():.2f} to {ap_coordinates['Y'].max():.2f}\")\n",
    "        if 'Z' in ap_coordinates.columns:\n",
    "            print(f\"Z: {ap_coordinates['Z'].min():.2f} to {ap_coordinates['Z'].max():.2f}\")\n",
    "else:\n",
    "    print(\"No AP coordinate data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3844d",
   "metadata": {},
   "source": [
    "## 6.1 Custom AP Position Layout\n",
    "\n",
    "Create custom AP positions to match your desired floor layout as shown in the scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c066bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_ap_positions():\n",
    "    \"\"\"Create custom AP positions to match the desired layout from scatter plots\"\"\"\n",
    "    \n",
    "    # Custom positions for Floor 2F (Z = 6.0)\n",
    "    # Based on the left scatter plot showing APs distributed across the floor\n",
    "    floor_2f_positions = [\n",
    "        # Left side APs\n",
    "        (\"D1_2F_AP001\", 90, 175, 6.0),\n",
    "        (\"D1_2F_AP002\", 100, 145, 6.0),\n",
    "        (\"D1_2F_AP003\", 95, 115, 6.0),\n",
    "        (\"D1_2F_AP004\", 90, 90, 6.0),\n",
    "        \n",
    "        # Center-left APs\n",
    "        (\"D1_2F_AP005\", 130, 145, 6.0),\n",
    "        (\"D1_2F_AP006\", 125, 115, 6.0),\n",
    "        (\"D1_2F_AP007\", 135, 90, 6.0),\n",
    "        \n",
    "        # Center APs\n",
    "        (\"D1_2F_AP008\", 320, 225, 6.0),\n",
    "        (\"D1_2F_AP009\", 340, 225, 6.0),\n",
    "        (\"D1_2F_AP010\", 360, 225, 6.0),\n",
    "        \n",
    "        # Right side APs\n",
    "        (\"D1_2F_AP011\", 480, 145, 6.0),\n",
    "        (\"D1_2F_AP012\", 490, 115, 6.0),\n",
    "        (\"D1_2F_AP013\", 500, 90, 6.0),\n",
    "        (\"D1_2F_AP014\", 520, 65, 6.0),\n",
    "        (\"D1_2F_AP015\", 500, 35, 6.0),\n",
    "        \n",
    "        # Top section\n",
    "        (\"D1_2F_AP016\", 110, 260, 6.0),\n",
    "        (\"D1_2F_AP017\", 130, 260, 6.0),\n",
    "        (\"D1_2F_AP018\", 520, 175, 6.0),\n",
    "        (\"D1_2F_AP019\", 540, 195, 6.0),\n",
    "        (\"D1_2F_AP020\", 520, 225, 6.0)\n",
    "    ]\n",
    "    \n",
    "    # Custom positions for Floor 3F (Z = 9.0)\n",
    "    # Based on the right scatter plot showing more APs in different pattern\n",
    "    floor_3f_positions = [\n",
    "        # Left cluster\n",
    "        (\"D1_3F_AP001\", 80, 195, 9.0),\n",
    "        (\"D1_3F_AP002\", 95, 170, 9.0),\n",
    "        (\"D1_3F_AP003\", 85, 140, 9.0),\n",
    "        (\"D1_3F_AP004\", 90, 110, 9.0),\n",
    "        (\"D1_3F_AP005\", 85, 85, 9.0),\n",
    "        \n",
    "        # Center-left column\n",
    "        (\"D1_3F_AP006\", 150, 140, 9.0),\n",
    "        (\"D1_3F_AP007\", 155, 110, 9.0),\n",
    "        (\"D1_3F_AP008\", 150, 85, 9.0),\n",
    "        \n",
    "        # Center area\n",
    "        (\"D1_3F_AP009\", 190, 215, 9.0),\n",
    "        (\"D1_3F_AP010\", 220, 215, 9.0),\n",
    "        (\"D1_3F_AP011\", 250, 215, 9.0),\n",
    "        (\"D1_3F_AP012\", 280, 215, 9.0),\n",
    "        (\"D1_3F_AP013\", 270, 270, 9.0),\n",
    "        (\"D1_3F_AP014\", 300, 270, 9.0),\n",
    "        \n",
    "        # Right side clusters\n",
    "        (\"D1_3F_AP015\", 360, 215, 9.0),\n",
    "        (\"D1_3F_AP016\", 390, 215, 9.0),\n",
    "        (\"D1_3F_AP017\", 420, 215, 9.0),\n",
    "        (\"D1_3F_AP018\", 450, 215, 9.0),\n",
    "        (\"D1_3F_AP019\", 480, 215, 9.0),\n",
    "        (\"D1_3F_AP020\", 510, 215, 9.0),\n",
    "        \n",
    "        # Bottom right\n",
    "        (\"D1_3F_AP021\", 380, 170, 9.0),\n",
    "        (\"D1_3F_AP022\", 410, 170, 9.0),\n",
    "        (\"D1_3F_AP023\", 440, 140, 9.0),\n",
    "        (\"D1_3F_AP024\", 470, 140, 9.0),\n",
    "        (\"D1_3F_AP025\", 500, 170, 9.0),\n",
    "        \n",
    "        # Additional scattered APs\n",
    "        (\"D1_3F_AP026\", 320, 140, 9.0),\n",
    "        (\"D1_3F_AP027\", 350, 140, 9.0),\n",
    "        (\"D1_3F_AP028\", 400, 110, 9.0),\n",
    "        (\"D1_3F_AP029\", 430, 110, 9.0),\n",
    "        (\"D1_3F_AP030\", 460, 85, 9.0),\n",
    "        (\"D1_3F_AP031\", 490, 85, 9.0),\n",
    "        (\"D1_3F_AP032\", 520, 85, 9.0),\n",
    "        \n",
    "        # Top area\n",
    "        (\"D1_3F_AP033\", 200, 270, 9.0),\n",
    "        (\"D1_3F_AP034\", 230, 270, 9.0),\n",
    "        (\"D1_3F_AP035\", 350, 270, 9.0),\n",
    "        (\"D1_3F_AP036\", 380, 270, 9.0),\n",
    "        (\"D1_3F_AP037\", 410, 270, 9.0),\n",
    "        (\"D1_3F_AP038\", 440, 270, 9.0),\n",
    "        (\"D1_3F_AP039\", 470, 270, 9.0),\n",
    "        (\"D1_3F_AP040\", 500, 270, 9.0),\n",
    "        \n",
    "        # Bottom scattered\n",
    "        (\"D1_3F_AP041\", 320, 55, 9.0),\n",
    "        (\"D1_3F_AP042\", 350, 55, 9.0),\n",
    "        (\"D1_3F_AP043\", 380, 30, 9.0),\n",
    "        (\"D1_3F_AP044\", 410, 30, 9.0),\n",
    "        (\"D1_3F_AP045\", 440, 55, 9.0),\n",
    "        (\"D1_3F_AP046\", 470, 55, 9.0),\n",
    "        (\"D1_3F_AP047\", 500, 30, 9.0),\n",
    "        (\"D1_3F_AP048\", 520, 55, 9.0)\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_2f = pd.DataFrame(floor_2f_positions, columns=['AP_Name', 'X', 'Y', 'Z'])\n",
    "    df_3f = pd.DataFrame(floor_3f_positions, columns=['AP_Name', 'X', 'Y', 'Z'])\n",
    "    \n",
    "    return df_2f, df_3f\n",
    "\n",
    "def save_custom_ap_positions():\n",
    "    \"\"\"Save the custom AP positions to CSV files\"\"\"\n",
    "    try:\n",
    "        df_2f, df_3f = create_custom_ap_positions()\n",
    "        \n",
    "        # Save to CSV files\n",
    "        df_2f.to_csv('2f.csv', index=False)\n",
    "        df_3f.to_csv('3f.csv', index=False)\n",
    "        \n",
    "        print(f\"✅ Created custom AP position files:\")\n",
    "        print(f\"  2f.csv: {len(df_2f)} APs\")\n",
    "        print(f\"  3f.csv: {len(df_3f)} APs\")\n",
    "        \n",
    "        # Show coordinate ranges\n",
    "        print(f\"\\nFloor 2F coordinate ranges:\")\n",
    "        print(f\"  X: {df_2f['X'].min():.0f} to {df_2f['X'].max():.0f}\")\n",
    "        print(f\"  Y: {df_2f['Y'].min():.0f} to {df_2f['Y'].max():.0f}\")\n",
    "        print(f\"  Z: {df_2f['Z'].unique()}\")\n",
    "        \n",
    "        print(f\"\\nFloor 3F coordinate ranges:\")\n",
    "        print(f\"  X: {df_3f['X'].min():.0f} to {df_3f['X'].max():.0f}\")\n",
    "        print(f\"  Y: {df_3f['Y'].min():.0f} to {df_3f['Y'].max():.0f}\")\n",
    "        print(f\"  Z: {df_3f['Z'].unique()}\")\n",
    "        \n",
    "        return df_2f, df_3f\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating custom positions: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Create and save custom AP positions\n",
    "print(\"Creating custom AP positions to match your scatter plot layout...\")\n",
    "df_2f_custom, df_3f_custom = save_custom_ap_positions()\n",
    "\n",
    "if df_2f_custom is not None and df_3f_custom is not None:\n",
    "    print(f\"\\n📍 AP Position Summary:\")\n",
    "    print(f\"Floor 2F: {len(df_2f_custom)} APs positioned\")\n",
    "    print(f\"Floor 3F: {len(df_3f_custom)} APs positioned\")\n",
    "    \n",
    "    # Display sample positions\n",
    "    print(f\"\\nSample Floor 2F positions:\")\n",
    "    display(df_2f_custom.head())\n",
    "    \n",
    "    print(f\"\\nSample Floor 3F positions:\")\n",
    "    display(df_3f_custom.head())\n",
    "else:\n",
    "    print(\"Failed to create custom positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_custom_ap_layout(df_2f, df_3f):\n",
    "    \"\"\"Visualize the custom AP layout to match your scatter plots\"\"\"\n",
    "    \n",
    "    # Create subplot for both floors\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot Floor 2F\n",
    "    ax1.scatter(df_2f['X'], df_2f['Y'], c='blue', s=50, alpha=0.7)\n",
    "    ax1.set_title('Scatter Plot of Positions (2F)')\n",
    "    ax1.set_xlabel('X Position')\n",
    "    ax1.set_ylabel('Y Position')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(50, 550)\n",
    "    ax1.set_ylim(25, 275)\n",
    "    \n",
    "    # Plot Floor 3F\n",
    "    ax2.scatter(df_3f['X'], df_3f['Y'], c='blue', s=50, alpha=0.7)\n",
    "    ax2.set_title('Scatter Plot of Positions (3F)')\n",
    "    ax2.set_xlabel('X Position')\n",
    "    ax2.set_ylabel('Y Position')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(50, 550)\n",
    "    ax2.set_ylim(25, 275)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📊 Custom AP Layout Visualization:\")\n",
    "    print(f\"Floor 2F: {len(df_2f)} APs distributed\")\n",
    "    print(f\"Floor 3F: {len(df_3f)} APs distributed\")\n",
    "\n",
    "# Visualize the custom layout if positions were created successfully\n",
    "if df_2f_custom is not None and df_3f_custom is not None:\n",
    "    visualize_custom_ap_layout(df_2f_custom, df_3f_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0edae5",
   "metadata": {},
   "source": [
    "## 📝 How to Modify AP Positions\n",
    "\n",
    "To customize your AP positions to match your desired layout:\n",
    "\n",
    "### 1. **Edit the Position Coordinates**\n",
    "In the `create_custom_ap_positions()` function above, modify the coordinates:\n",
    "```python\n",
    "# Format: (AP_Name, X_coordinate, Y_coordinate, Z_coordinate)\n",
    "(\"D1_2F_AP001\", 90, 175, 6.0),  # X=90, Y=175, Floor 2 (Z=6.0)\n",
    "(\"D1_3F_AP001\", 80, 195, 9.0),  # X=80, Y=195, Floor 3 (Z=9.0)\n",
    "```\n",
    "\n",
    "### 2. **Coordinate System**\n",
    "- **X-axis**: 0-550 (building width)\n",
    "- **Y-axis**: 0-275 (building length) \n",
    "- **Z-axis**: 6.0 for Floor 2F, 9.0 for Floor 3F\n",
    "\n",
    "### 3. **Add/Remove APs**\n",
    "- Add more APs by adding tuples to the lists\n",
    "- Remove APs by deleting tuples\n",
    "- Change AP names to match your actual AP naming convention\n",
    "\n",
    "### 4. **Apply Changes**\n",
    "After modifying coordinates:\n",
    "1. Run the `save_custom_ap_positions()` function\n",
    "2. This creates new `2f.csv` and `3f.csv` files\n",
    "3. Use these files in your main pipeline\n",
    "\n",
    "### 5. **Use in Main Pipeline**\n",
    "The generated CSV files will be automatically used by your `run_all.bash` script when it processes the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b0aa8",
   "metadata": {},
   "source": [
    "## 7. Merge AP Location with Signal Data\n",
    "\n",
    "Merge AP coordinate information with the signal strength DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ap_locations(df_signal, df_coords):\n",
    "    \"\"\"Merge AP location data with signal strength data\"\"\"\n",
    "    if df_signal is None or df_coords is None:\n",
    "        print(\"Cannot merge: missing signal data or coordinate data\")\n",
    "        return df_signal\n",
    "    \n",
    "    try:\n",
    "        # Get RSSI columns that correspond to APs\n",
    "        rssi_columns = [col for col in df_signal.columns if col.startswith('rssi_')]\n",
    "        \n",
    "        if not rssi_columns:\n",
    "            print(\"No RSSI columns found for merging\")\n",
    "            return df_signal\n",
    "        \n",
    "        # Extract AP names from RSSI column names\n",
    "        ap_names_in_signal = [col.replace('rssi_', '') for col in rssi_columns]\n",
    "        print(f\"APs in signal data: {len(ap_names_in_signal)}\")\n",
    "        print(f\"Sample AP names: {ap_names_in_signal[:5]}\")\n",
    "        \n",
    "        # Create a mapping between AP names in signal data and coordinate data\n",
    "        coord_ap_names = df_coords['AP_Name'].tolist()\n",
    "        print(f\"APs in coordinate data: {len(coord_ap_names)}\")\n",
    "        print(f\"Sample coordinate AP names: {coord_ap_names[:5]}\")\n",
    "        \n",
    "        # Find matching APs\n",
    "        matched_aps = []\n",
    "        ap_coordinates_map = {}\n",
    "        \n",
    "        for signal_ap in ap_names_in_signal:\n",
    "            # Try exact match first\n",
    "            if signal_ap in coord_ap_names:\n",
    "                matched_aps.append(signal_ap)\n",
    "                coord_row = df_coords[df_coords['AP_Name'] == signal_ap].iloc[0]\n",
    "                ap_coordinates_map[signal_ap] = {\n",
    "                    'X': coord_row['X'],\n",
    "                    'Y': coord_row['Y'],\n",
    "                    'Z': coord_row.get('Z', 0)\n",
    "                }\n",
    "            else:\n",
    "                # Try partial matching (e.g., case insensitive or substring)\n",
    "                for coord_ap in coord_ap_names:\n",
    "                    if signal_ap.lower() in coord_ap.lower() or coord_ap.lower() in signal_ap.lower():\n",
    "                        matched_aps.append(signal_ap)\n",
    "                        coord_row = df_coords[df_coords['AP_Name'] == coord_ap].iloc[0]\n",
    "                        ap_coordinates_map[signal_ap] = {\n",
    "                            'X': coord_row['X'],\n",
    "                            'Y': coord_row['Y'],\n",
    "                            'Z': coord_row.get('Z', 0)\n",
    "                        }\n",
    "                        break\n",
    "        \n",
    "        print(f\\\"Matched APs: {len(matched_aps)}\\\")\\n        print(f\\\"Match rate: {len(matched_aps)/len(ap_names_in_signal)*100:.1f}%\\\")\\n        \\n        if matched_aps:\\n            # Create coordinate columns for matched APs\\n            df_merged = df_signal.copy()\\n            \\n            # Add coordinate information for each matched AP\\n            for ap_name in matched_aps:\\n                rssi_col = f'rssi_{ap_name}'\\n                if rssi_col in df_merged.columns:\\n                    coords = ap_coordinates_map[ap_name]\\n                    df_merged[f'{rssi_col}_X'] = coords['X']\\n                    df_merged[f'{rssi_col}_Y'] = coords['Y']\\n                    df_merged[f'{rssi_col}_Z'] = coords['Z']\\n            \\n            print(f\\\"Added coordinate columns for {len(matched_aps)} APs\\\")\\n            print(f\\\"Final merged data shape: {df_merged.shape}\\\")\\n            return df_merged\\n        else:\\n            print(\\\"No matching APs found between signal and coordinate data\\\")\\n            return df_signal\\n            \\n    except Exception as e:\\n        print(f\\\"Error merging AP locations: {str(e)}\\\")\\n        return df_signal\\n\\n# Merge AP locations with signal data\\nif df_pivoted is not None and ap_coordinates is not None:\\n    df_final = merge_ap_locations(df_pivoted, ap_coordinates)\\n    \\n    if df_final is not None:\\n        print(f\\\"\\\\nFinal merged dataset:\\\")\\n        print(f\\\"Shape: {df_final.shape}\\\")\\n        \\n        # Show column types\\n        rssi_cols = [col for col in df_final.columns if col.startswith('rssi_') and not col.endswith(('_X', '_Y', '_Z'))]\\n        coord_cols = [col for col in df_final.columns if col.endswith(('_X', '_Y', '_Z'))]\\n        other_cols = [col for col in df_final.columns if col not in rssi_cols and col not in coord_cols]\\n        \\n        print(f\\\"RSSI columns: {len(rssi_cols)}\\\")\\n        print(f\\\"Coordinate columns: {len(coord_cols)}\\\")\\n        print(f\\\"Other columns: {len(other_cols)}\\\")\\n        \\n        print(f\\\"\\\\nSample merged data:\\\")\\n        display(df_final.head())\\n    else:\\n        print(\\\"Merging failed\\\")\\n        df_final = df_pivoted\\nelse:\\n    print(\\\"Cannot merge: missing pivoted data or coordinate data\\\")\\n    df_final = df_pivoted if df_pivoted is not None else df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901d9b4",
   "metadata": {},
   "source": [
    "## 8. Export Processed Data to CSV\n",
    "\n",
    "Save the final processed and merged dataset to CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e06d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(df, filename):\n",
    "    \"\"\"Export DataFrame to CSV file\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to export\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(filename) if os.path.dirname(filename) else '.'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        print(f\"Data exported successfully to: {filename}\")\n",
    "        print(f\"File size: {os.path.getsize(filename) / 1024:.2f} KB\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        \n",
    "        # Show summary statistics\n",
    "        rssi_cols = [col for col in df.columns if col.startswith('rssi_') and not col.endswith(('_X', '_Y', '_Z'))]\n",
    "        if rssi_cols:\n",
    "            print(f\"\\\\nRSSI Statistics:\")\n",
    "            rssi_data = df[rssi_cols]\n",
    "            print(f\"  Non-null RSSI values: {rssi_data.count().sum()}\\\")\\n            print(f\\\"  Average RSSI range: {rssi_data.min().min():.1f} to {rssi_data.max().max():.1f} dBm\\\")\\n            print(f\\\"  Coverage: {(rssi_data.count() / len(df) * 100).mean():.1f}% average\\\")\\n        \\n        return True\\n        \\n    except Exception as e:\\n        print(f\\\"Error exporting data: {str(e)}\\\")\\n        return False\\n\\n# Export the final processed data\\nif df_final is not None:\\n    success = export_to_csv(df_final, OUTPUT_FILE)\\n    \\n    if success:\\n        print(f\\\"\\\\n✅ Data acquisition process completed successfully!\\\")\\n        print(f\\\"\\\\nFinal dataset summary:\\\")\\n        print(f\\\"  Output file: {OUTPUT_FILE}\\\")\\n        print(f\\\"  Total records: {len(df_final)}\\\")\\n        print(f\\\"  Total columns: {len(df_final.columns)}\\\")\\n        \\n        # Categorize columns\\n        rssi_cols = [col for col in df_final.columns if col.startswith('rssi_') and not col.endswith(('_X', '_Y', '_Z'))]\\n        coord_cols = [col for col in df_final.columns if col.endswith(('_X', '_Y', '_Z'))]\\n        other_cols = [col for col in df_final.columns if col not in rssi_cols and col not in coord_cols]\\n        \\n        print(f\\\"  RSSI measurements: {len(rssi_cols)} APs\\\")\\n        print(f\\\"  Coordinate data: {len(coord_cols)//3 if coord_cols else 0} APs with locations\\\")\\n        print(f\\\"  Metadata columns: {len(other_cols)}\\\")\\n        \\n        print(f\\\"\\\\nColumn breakdown:\\\")\\n        if other_cols:\\n            print(f\\\"  Metadata: {other_cols}\\\")\\n        if rssi_cols:\\n            print(f\\\"  RSSI columns (first 5): {rssi_cols[:5]}\\\")\\n        if coord_cols:\\n            print(f\\\"  Coordinate columns (first 6): {coord_cols[:6]}\\\")\\n    else:\\n        print(\\\"❌ Export failed\\\")\\nelse:\\n    print(\\\"❌ No final data available for export\\\")\\n\\nprint(f\\\"\\\\n=== Data Acquisition Process Complete ===\\\")\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
