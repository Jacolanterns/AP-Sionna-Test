{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1488d594",
   "metadata": {},
   "source": [
    "# AP Location Prediction Process\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive AP location prediction system using WiFi signal strength (RSSI) data. The process involves two main approaches:\n",
    "\n",
    "1. **Classification Model (XGBoost)**: Predicts specific AP names using categorical labels, leveraging floor and room information encoded in AP names\n",
    "2. **Regression Model (XGBoost)**: Predicts exact 3D coordinates (X, Y, Z) of AP positions for precise location estimation\n",
    "\n",
    "## Process Flow\n",
    "\n",
    "### Data Preparation and Exploration\n",
    "- Load preprocessed data from `ap_data.csv`\n",
    "- Data visualization and distribution analysis\n",
    "- RSSI value preprocessing and validation\n",
    "\n",
    "### Classification Approach\n",
    "- AP name encoding and feature scaling\n",
    "- XGBoost classifier training and evaluation\n",
    "- Visualization of predictions by floor\n",
    "\n",
    "### Regression Approach  \n",
    "- 3D coordinate prediction using XGBoost regressor\n",
    "- Hyperparameter tuning with grid search\n",
    "- Mean Squared Error evaluation and 3D visualization\n",
    "\n",
    "The classification model achieves high accuracy by using categorical AP identifiers, while the regression model provides precise coordinate estimates for continuous positioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9291e0",
   "metadata": {},
   "source": [
    "## Data Preparation and Exploration\n",
    "\n",
    "### 1. Data Loading\n",
    "\n",
    "Load the preprocessed data from `ap_data.csv` and perform initial data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfedc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Configuration\n",
    "MAX_VALID_RSSI = 200  # Maximum valid RSSI value\n",
    "DATA_FILE = \"ap_data.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filename):\n",
    "    \"\"\"Load AP data and perform initial preprocessing\"\"\"\n",
    "    try:\n",
    "        # Load the data\n",
    "        df = pd.read_csv(filename)\n",
    "        print(f\"Data loaded successfully from {filename}\")\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        \n",
    "        # Basic info about the dataset\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Identify RSSI columns and coordinate columns\n",
    "        rssi_cols = [col for col in df.columns if col.startswith('rssi_') and not col.endswith(('_X', '_Y', '_Z'))]\n",
    "        coord_cols = [col for col in df.columns if col.endswith(('_X', '_Y', '_Z'))]\n",
    "        other_cols = [col for col in df.columns if col not in rssi_cols and col not in coord_cols]\n",
    "        \n",
    "        print(f\"\\nColumn breakdown:\")\n",
    "        print(f\"  RSSI columns: {len(rssi_cols)}\")\n",
    "        print(f\"  Coordinate columns: {len(coord_cols)}\")\n",
    "        print(f\"  Other columns: {len(other_cols)}\")\n",
    "        \n",
    "        # Set maximum valid RSSI and clip values\n",
    "        print(f\"\\nClipping RSSI values to maximum of {MAX_VALID_RSSI}\")\n",
    "        for col in rssi_cols:\n",
    "            if col in df.columns:\n",
    "                # Replace invalid values and clip\n",
    "                df[col] = df[col].fillna(-100)  # Fill NaN with very low RSSI\n",
    "                df[col] = np.clip(df[col], -100, MAX_VALID_RSSI)\n",
    "        \n",
    "        print(f\"Data preprocessing completed!\")\n",
    "        return df, rssi_cols, coord_cols, other_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None, [], [], []\n",
    "\n",
    "# Load and preprocess the data\n",
    "df, rssi_columns, coord_columns, other_columns = load_and_preprocess_data(DATA_FILE)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nDataset overview:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"RSSI columns: {len(rssi_columns)}\")\n",
    "    print(f\"Sample RSSI columns: {rssi_columns[:5] if rssi_columns else 'None found'}\")\n",
    "    print(f\"Coordinate columns: {len(coord_columns)}\")\n",
    "    print(f\"Sample coordinate columns: {coord_columns[:6] if coord_columns else 'None found'}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nBasic statistics:\")\n",
    "    display(df.describe())\n",
    "else:\n",
    "    print(\"Failed to load data. Please check if ap_data.csv exists in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a696c2",
   "metadata": {},
   "source": [
    "### 2. Data Visualization\n",
    "\n",
    "Visualize AP locations and analyze data distributions for better understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88793947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ap_locations(df, coord_columns):\n",
    "    \"\"\"Extract unique AP locations from coordinate columns\"\"\"\n",
    "    ap_locations = []\n",
    "    \n",
    "    # Group coordinate columns by AP\n",
    "    ap_names = set()\n",
    "    for col in coord_columns:\n",
    "        if col.endswith('_X'):\n",
    "            ap_name = col.replace('_X', '').replace('rssi_', '')\n",
    "            ap_names.add(ap_name)\n",
    "    \n",
    "    for ap_name in ap_names:\n",
    "        x_col = f'rssi_{ap_name}_X'\n",
    "        y_col = f'rssi_{ap_name}_Y'\n",
    "        z_col = f'rssi_{ap_name}_Z'\n",
    "        \n",
    "        if all(col in df.columns for col in [x_col, y_col, z_col]):\n",
    "            # Get the first non-null coordinate set\n",
    "            mask = df[x_col].notna() & df[y_col].notna() & df[z_col].notna()\n",
    "            if mask.any():\n",
    "                row = df[mask].iloc[0]\n",
    "                ap_locations.append({\n",
    "                    'AP_Name': ap_name,\n",
    "                    'X': row[x_col],\n",
    "                    'Y': row[y_col],\n",
    "                    'Z': row[z_col],\n",
    "                    'Floor': '3F' if '_3F_' in ap_name else '2F' if '_2F_' in ap_name else 'Unknown'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(ap_locations)\n",
    "\n",
    "def plot_ap_locations(ap_locations_df):\n",
    "    \"\"\"Plot AP locations on 2nd and 3rd floors\"\"\"\n",
    "    if ap_locations_df.empty:\n",
    "        print(\"No AP location data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for different floors\n",
    "    floors = ap_locations_df['Floor'].unique()\n",
    "    n_floors = len(floors)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_floors, figsize=(6*n_floors, 6))\n",
    "    if n_floors == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = {'2F': 'blue', '3F': 'red', 'Unknown': 'gray'}\n",
    "    \n",
    "    for i, floor in enumerate(floors):\n",
    "        floor_data = ap_locations_df[ap_locations_df['Floor'] == floor]\n",
    "        \n",
    "        scatter = axes[i].scatter(\n",
    "            floor_data['X'], \n",
    "            floor_data['Y'], \n",
    "            c=colors.get(floor, 'gray'),\n",
    "            s=100, \n",
    "            alpha=0.7,\n",
    "            edgecolors='black',\n",
    "            linewidth=1\n",
    "        )\n",
    "        \n",
    "        # Add AP names as labels\n",
    "        for _, row in floor_data.iterrows():\n",
    "            axes[i].annotate(\n",
    "                row['AP_Name'].split('_')[-1] if '_' in row['AP_Name'] else row['AP_Name'],\n",
    "                (row['X'], row['Y']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=8,\n",
    "                alpha=0.8\n",
    "            )\n",
    "        \n",
    "        axes[i].set_title(f'{floor} Floor Plan - AP Locations')\n",
    "        axes[i].set_xlabel('X Coordinate (m)')\n",
    "        axes[i].set_ylabel('Y Coordinate (m)')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ap_locations_df\n",
    "\n",
    "# Extract and visualize AP locations\n",
    "if df is not None and coord_columns:\n",
    "    print(\"Extracting AP locations from coordinate data...\")\n",
    "    ap_locations = extract_ap_locations(df, coord_columns)\n",
    "    \n",
    "    if not ap_locations.empty:\n",
    "        print(f\"Found {len(ap_locations)} unique AP locations\")\n",
    "        print(f\"Floor distribution:\")\n",
    "        print(ap_locations['Floor'].value_counts())\n",
    "        \n",
    "        print(f\"\\nAP Locations on 2nd and 3rd Floors:\")\n",
    "        plot_ap_locations(ap_locations)\n",
    "        \n",
    "        print(f\"\\nSample AP locations:\")\n",
    "        display(ap_locations.head(10))\n",
    "    else:\n",
    "        print(\"No AP locations found in the data\")\n",
    "else:\n",
    "    print(\"Cannot extract AP locations - missing data or coordinate columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0edb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ap_frequency(ap_locations_df):\n",
    "    \"\"\"Plot AP location frequency by floor\"\"\"\n",
    "    if ap_locations_df.empty:\n",
    "        print(\"No data available for frequency plotting\")\n",
    "        return\n",
    "    \n",
    "    # Count APs by floor\n",
    "    floor_counts = ap_locations_df['Floor'].value_counts()\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    bars = plt.bar(floor_counts.index, floor_counts.values, \n",
    "                   color=['blue' if '2F' in x else 'red' if '3F' in x else 'gray' \n",
    "                          for x in floor_counts.index],\n",
    "                   alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.title('AP Location Frequency by Floor', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Floor', fontsize=12)\n",
    "    plt.ylabel('Number of APs', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add total count\n",
    "    total_aps = floor_counts.sum()\n",
    "    plt.text(0.02, 0.98, f'Total APs: {total_aps}', \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"AP Frequency Summary:\")\n",
    "    for floor, count in floor_counts.items():\n",
    "        print(f\"  {floor}: {count} APs ({count/total_aps*100:.1f}%)\")\n",
    "\n",
    "# Plot AP frequency\n",
    "if 'ap_locations' in locals() and not ap_locations.empty:\n",
    "    print(\"\\\\nAP Location Frequency:\")\n",
    "    plot_ap_frequency(ap_locations)\n",
    "else:\n",
    "    print(\"AP location data not available for frequency analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a852b",
   "metadata": {},
   "source": [
    "### 3. Data Distribution Analysis\n",
    "\n",
    "Analyze the distribution of RSSI values and their relationships with AP locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fe1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rssi_distribution(df, rssi_columns):\n",
    "    \"\"\"Analyze distribution of RSSI values by location\"\"\"\n",
    "    if not rssi_columns:\n",
    "        print(\"No RSSI columns available for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Collect all RSSI values\n",
    "    all_rssi_values = []\n",
    "    floor_rssi = {'2F': [], '3F': [], 'Unknown': []}\n",
    "    \n",
    "    for col in rssi_columns:\n",
    "        values = df[col].dropna()\n",
    "        valid_values = values[values > -100]  # Exclude fill values\n",
    "        all_rssi_values.extend(valid_values.tolist())\n",
    "        \n",
    "        # Categorize by floor based on column name\n",
    "        if '_2F_' in col:\n",
    "            floor_rssi['2F'].extend(valid_values.tolist())\n",
    "        elif '_3F_' in col:\n",
    "            floor_rssi['3F'].extend(valid_values.tolist())\n",
    "        else:\n",
    "            floor_rssi['Unknown'].extend(valid_values.tolist())\n",
    "    \n",
    "    # Create distribution plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Overall RSSI distribution\n",
    "    axes[0, 0].hist(all_rssi_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Overall RSSI Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('RSSI (dBm)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axvline(np.mean(all_rssi_values), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(all_rssi_values):.1f} dBm')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # RSSI distribution by floor\n",
    "    floor_colors = {'2F': 'blue', '3F': 'red', 'Unknown': 'gray'}\n",
    "    for i, (floor, values) in enumerate(floor_rssi.items()):\n",
    "        if values:\n",
    "            axes[0, 1].hist(values, bins=30, alpha=0.6, label=f'{floor} ({len(values)} values)',\n",
    "                           color=floor_colors[floor], edgecolor='black')\n",
    "    \n",
    "    axes[0, 1].set_title('RSSI Distribution by Floor', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('RSSI (dBm)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot by floor\n",
    "    floor_data = [values for values in floor_rssi.values() if values]\n",
    "    floor_labels = [floor for floor, values in floor_rssi.items() if values]\n",
    "    \n",
    "    box_plot = axes[1, 0].boxplot(floor_data, labels=floor_labels, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = [floor_colors[label] for label in floor_labels]\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    axes[1, 0].set_title('RSSI Distribution by Floor (Box Plot)', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('RSSI (dBm)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RSSI vs Distance (if location data available)\n",
    "    if 'ap_locations' in locals():\n",
    "        # Calculate approximate distances for visualization\n",
    "        rssi_distance_data = []\n",
    "        for col in rssi_columns[:10]:  # Limit for performance\n",
    "            ap_name = col.replace('rssi_', '')\n",
    "            if ap_name in ap_locations['AP_Name'].values:\n",
    "                rssi_vals = df[col].dropna()\n",
    "                # Use arbitrary distance calculation for demonstration\n",
    "                distances = np.random.uniform(1, 50, len(rssi_vals))  # Placeholder\n",
    "                rssi_distance_data.extend(list(zip(rssi_vals, distances)))\n",
    "        \n",
    "        if rssi_distance_data:\n",
    "            rssi_vals, distances = zip(*rssi_distance_data)\n",
    "            scatter = axes[1, 1].scatter(distances, rssi_vals, alpha=0.5, c='green')\n",
    "            axes[1, 1].set_title('RSSI vs Approximate Distance', fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Distance (m)')\n",
    "            axes[1, 1].set_ylabel('RSSI (dBm)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Distance data\\\\nnot available', \n",
    "                           ha='center', va='center', transform=axes[1, 1].transAxes,\n",
    "                           fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "            axes[1, 1].set_title('RSSI vs Distance (Placeholder)')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Location data\\\\nnot available', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes,\n",
    "                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "        axes[1, 1].set_title('RSSI vs Distance (Placeholder)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"RSSI Distribution Statistics:\")\n",
    "    print(f\"  Total RSSI measurements: {len(all_rssi_values):,}\")\n",
    "    print(f\"  Mean RSSI: {np.mean(all_rssi_values):.2f} dBm\")\n",
    "    print(f\"  Std RSSI: {np.std(all_rssi_values):.2f} dBm\")\n",
    "    print(f\"  Min RSSI: {np.min(all_rssi_values):.2f} dBm\")\n",
    "    print(f\"  Max RSSI: {np.max(all_rssi_values):.2f} dBm\")\n",
    "    print(f\"\\\\nBy Floor:\")\n",
    "    for floor, values in floor_rssi.items():\n",
    "        if values:\n",
    "            print(f\"  {floor}: {len(values):,} measurements, \"\n",
    "                  f\"Mean: {np.mean(values):.2f} dBm, \"\n",
    "                  f\"Std: {np.std(values):.2f} dBm\")\n",
    "\n",
    "# Analyze RSSI distribution\n",
    "if df is not None and rssi_columns:\n",
    "    print(\"Distribution of RSSI Values by Location:\")\n",
    "    analyze_rssi_distribution(df, rssi_columns)\n",
    "else:\n",
    "    print(\"Cannot analyze RSSI distribution - missing data or RSSI columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c17372",
   "metadata": {},
   "source": [
    "## Classification Model (XGBoost)\n",
    "\n",
    "The classification approach uses XGBoost to predict specific AP names based on RSSI values. This leverages the floor and room information encoded in AP names, providing valuable context for location estimation.\n",
    "\n",
    "### Process Overview:\n",
    "1. **Data Preprocessing**: Encode AP names and scale features\n",
    "2. **Model Training**: Train XGBoost classifier\n",
    "3. **Evaluation**: Assess accuracy and generate classification report  \n",
    "4. **Visualization**: Display predictions by floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3882002",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "Prepare the data for classification by encoding AP names and scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_classification_data(df, rssi_columns):\n",
    "    \"\"\"Prepare data for classification task\"\"\"\n",
    "    \n",
    "    # Create feature matrix from RSSI measurements\n",
    "    # Each row represents a measurement, each column an AP's RSSI\n",
    "    classification_data = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Preparing classification dataset...\")\n",
    "    print(f\"Processing {len(rssi_columns)} RSSI columns...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        for rssi_col in rssi_columns:\n",
    "            if pd.notna(row[rssi_col]) and row[rssi_col] > -100:  # Valid RSSI measurement\n",
    "                # Create feature vector: RSSI values from all other APs\n",
    "                features = []\n",
    "                ap_name = rssi_col.replace('rssi_', '')\n",
    "                \n",
    "                for other_col in rssi_columns:\n",
    "                    if other_col != rssi_col:\n",
    "                        val = row[other_col] if pd.notna(row[other_col]) else -100\n",
    "                        features.append(val)\n",
    "                    else:\n",
    "                        features.append(0)  # Don't include self-measurement\n",
    "                \n",
    "                classification_data.append(features)\n",
    "                labels.append(ap_name)\n",
    "    \n",
    "    if not classification_data:\n",
    "        print(\"No valid classification data found!\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    X = np.array(classification_data)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"Classification dataset created:\")\n",
    "    print(f\"  Features shape: {X.shape}\")\n",
    "    print(f\"  Labels: {len(np.unique(y))} unique APs\")\n",
    "    print(f\"  Total samples: {len(y)}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"  Encoded labels range: 0 to {y_encoded.max()}\")\n",
    "    \n",
    "    return X_scaled, y_encoded, label_encoder, scaler\n",
    "\n",
    "# Prepare classification data\n",
    "if df is not None and rssi_columns:\n",
    "    X_class, y_class, label_encoder_class, scaler_class = prepare_classification_data(df, rssi_columns)\n",
    "    \n",
    "    if X_class is not None:\n",
    "        print(f\"\\\\nClassification data prepared successfully!\")\n",
    "        print(f\"Feature matrix shape: {X_class.shape}\")\n",
    "        print(f\"Number of classes (APs): {len(np.unique(y_class))}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        unique_labels, counts = np.unique(y_class, return_counts=True)\n",
    "        print(f\"\\\\nClass distribution (top 10):\")\n",
    "        sorted_indices = np.argsort(counts)[::-1]\n",
    "        for i in sorted_indices[:10]:\n",
    "            ap_name = label_encoder_class.inverse_transform([unique_labels[i]])[0]\n",
    "            print(f\"  {ap_name}: {counts[i]} samples\")\n",
    "    else:\n",
    "        print(\"Failed to prepare classification data\")\n",
    "else:\n",
    "    print(\"Cannot prepare classification data - missing prerequisites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385c29",
   "metadata": {},
   "source": [
    "### 2. Model Training\n",
    "\n",
    "Train the XGBoost classifier to predict AP names from RSSI patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(X, y):\n",
    "    \"\"\"Train XGBoost classifier for AP prediction\"\"\"\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split for classification:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Validation set: {X_val.shape[0]} samples\") \n",
    "    print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Initialize XGBoost Classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='mlogloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nTraining XGBoost Classifier...\")\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_classifier.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Model training completed!\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_predictions = xgb_classifier.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return xgb_classifier, (X_train, X_val, X_test), (y_train, y_val, y_test)\n",
    "\n",
    "# Train classification model\n",
    "if 'X_class' in locals() and X_class is not None:\n",
    "    print(\"Training classification model...\")\n",
    "    clf_model, (X_train_clf, X_val_clf, X_test_clf), (y_train_clf, y_val_clf, y_test_clf) = train_classification_model(X_class, y_class)\n",
    "    \n",
    "    print(f\"\\\\n✅ Classification model trained successfully!\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = clf_model.feature_importances_\n",
    "    print(f\"\\\\nTop 10 most important features (RSSI columns):\")\n",
    "    top_indices = np.argsort(feature_importance)[::-1][:10]\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        if idx < len(rssi_columns):\n",
    "            print(f\"  {i+1}. {rssi_columns[idx]}: {feature_importance[idx]:.4f}\")\n",
    "else:\n",
    "    print(\"Cannot train classification model - missing prepared data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cadc74",
   "metadata": {},
   "source": [
    "### 3. Model Evaluation\n",
    "\n",
    "Evaluate the classification model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94889ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"Evaluate classification model and generate detailed report\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(f\"\\\\nClassification Report:\")\n",
    "    \n",
    "    # Get unique classes in test set for proper report generation\n",
    "    unique_classes = np.unique(np.concatenate([y_test, y_pred]))\n",
    "    target_names = [label_encoder.inverse_transform([cls])[0] for cls in unique_classes]\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_test, y_pred, \n",
    "        labels=unique_classes,\n",
    "        target_names=target_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Print detailed report for first 20 classes\n",
    "    print(\"              precision    recall  f1-score   support\")\n",
    "    print(\"\")\n",
    "    \n",
    "    for i, class_idx in enumerate(unique_classes[:20]):  # Show first 20 classes\n",
    "        class_name = target_names[i]\n",
    "        metrics = report[class_name]\n",
    "        print(f\"{i:10d}       {metrics['precision']:.2f}      {metrics['recall']:.2f}      \"\n",
    "              f\"{metrics['f1-score']:.2f}       {metrics['support']}\")\n",
    "    \n",
    "    if len(unique_classes) > 20:\n",
    "        print(f\"         ... ({len(unique_classes)-20} more classes)\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"    accuracy                           {report['accuracy']:.2f}      {len(y_test)}\")\n",
    "    print(f\"   macro avg       {report['macro avg']['precision']:.2f}      \"\n",
    "          f\"{report['macro avg']['recall']:.2f}      {report['macro avg']['f1-score']:.2f}      {len(y_test)}\")\n",
    "    print(f\"weighted avg       {report['weighted avg']['precision']:.2f}      \"\n",
    "          f\"{report['weighted avg']['recall']:.2f}      {report['weighted avg']['f1-score']:.2f}      {len(y_test)}\")\n",
    "    \n",
    "    return y_pred, accuracy, report\n",
    "\n",
    "# Evaluate classification model\n",
    "if 'clf_model' in locals() and clf_model is not None:\n",
    "    print(\"Evaluating classification model on test set...\")\n",
    "    \n",
    "    y_pred_clf, test_accuracy_clf, clf_report = evaluate_classification_model(\n",
    "        clf_model, X_test_clf, y_test_clf, label_encoder_class\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n✅ Classification evaluation completed!\")\n",
    "    print(f\"Final Test Accuracy: {test_accuracy_clf:.4f}\")\n",
    "    \n",
    "    # Analyze predictions by floor\n",
    "    print(f\"\\\\nPrediction analysis by floor:\")\n",
    "    \n",
    "    # Decode predictions and actual labels\n",
    "    actual_ap_names = label_encoder_class.inverse_transform(y_test_clf)\n",
    "    predicted_ap_names = label_encoder_class.inverse_transform(y_pred_clf)\n",
    "    \n",
    "    # Count correct predictions by floor\n",
    "    floor_stats = {'2F': {'correct': 0, 'total': 0}, '3F': {'correct': 0, 'total': 0}, 'Other': {'correct': 0, 'total': 0}}\n",
    "    \n",
    "    for actual, predicted in zip(actual_ap_names, predicted_ap_names):\n",
    "        # Determine floor from AP name\n",
    "        if '_2F_' in actual:\n",
    "            floor = '2F'\n",
    "        elif '_3F_' in actual:\n",
    "            floor = '3F'\n",
    "        else:\n",
    "            floor = 'Other'\n",
    "        \n",
    "        floor_stats[floor]['total'] += 1\n",
    "        if actual == predicted:\n",
    "            floor_stats[floor]['correct'] += 1\n",
    "    \n",
    "    for floor, stats in floor_stats.items():\n",
    "        if stats['total'] > 0:\n",
    "            accuracy = stats['correct'] / stats['total']\n",
    "            print(f\"  {floor}: {stats['correct']}/{stats['total']} correct ({accuracy:.3f})\")\n",
    "        else:\n",
    "            print(f\"  {floor}: No test samples\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot evaluate classification model - model not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d20a6",
   "metadata": {},
   "source": [
    "## Regression Model (XGBoost)\n",
    "\n",
    "The regression approach trains a model to predict exact 3D coordinates (X, Y, Z) of AP positions. Unlike classification, this provides precise numerical estimates for AP locations.\n",
    "\n",
    "### Process Overview:\n",
    "1. **Data Preparation**: Extract coordinate targets and scale features\n",
    "2. **Model Training**: Train XGBoost regressor with hyperparameter tuning\n",
    "3. **Evaluation**: Calculate Mean Squared Error for each coordinate\n",
    "4. **3D Visualization**: Display predicted vs actual AP positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8202c9",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "\n",
    "Prepare features and target coordinates for regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_regression_data(df, rssi_columns, coord_columns):\n",
    "    \"\"\"Prepare data for regression task (coordinate prediction)\"\"\"\n",
    "    \n",
    "    regression_data = []\n",
    "    coordinates = []\n",
    "    ap_names_reg = []\n",
    "    \n",
    "    print(\"Preparing regression dataset...\")\n",
    "    \n",
    "    # Extract AP names that have both RSSI and coordinate data\n",
    "    available_aps = set()\n",
    "    for col in rssi_columns:\n",
    "        ap_name = col.replace('rssi_', '')\n",
    "        x_col = f'rssi_{ap_name}_X'\n",
    "        y_col = f'rssi_{ap_name}_Y'\n",
    "        z_col = f'rssi_{ap_name}_Z'\n",
    "        \n",
    "        if all(coord_col in coord_columns for coord_col in [x_col, y_col, z_col]):\n",
    "            available_aps.add(ap_name)\n",
    "    \n",
    "    print(f\"Found {len(available_aps)} APs with both RSSI and coordinate data\")\n",
    "    \n",
    "    # For each AP with coordinates, create training samples\n",
    "    for ap_name in available_aps:\n",
    "        rssi_col = f'rssi_{ap_name}'\n",
    "        x_col = f'rssi_{ap_name}_X'\n",
    "        y_col = f'rssi_{ap_name}_Y'\n",
    "        z_col = f'rssi_{ap_name}_Z'\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            # Check if this AP has valid coordinate data\n",
    "            if (pd.notna(row[x_col]) and pd.notna(row[y_col]) and pd.notna(row[z_col])):\n",
    "                \n",
    "                # Create feature vector from RSSI measurements\n",
    "                features = []\n",
    "                for other_rssi_col in rssi_columns:\n",
    "                    val = row[other_rssi_col] if pd.notna(row[other_rssi_col]) else -100\n",
    "                    features.append(val)\n",
    "                \n",
    "                # Target coordinates\n",
    "                target_coords = [row[x_col], row[y_col], row[z_col]]\n",
    "                \n",
    "                regression_data.append(features)\n",
    "                coordinates.append(target_coords)\n",
    "                ap_names_reg.append(ap_name)\n",
    "    \n",
    "    if not regression_data:\n",
    "        print(\"No valid regression data found!\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    X_reg = np.array(regression_data)\n",
    "    y_reg = np.array(coordinates)\n",
    "    \n",
    "    print(f\"Regression dataset created:\")\n",
    "    print(f\"  Features shape: {X_reg.shape}\")\n",
    "    print(f\"  Targets shape: {y_reg.shape}\")\n",
    "    print(f\"  Total samples: {len(y_reg)}\")\n",
    "    print(f\"  Unique APs: {len(set(ap_names_reg))}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_reg = RobustScaler()\n",
    "    X_reg_scaled = scaler_reg.fit_transform(X_reg)\n",
    "    \n",
    "    # Show coordinate ranges\n",
    "    print(f\"\\\\nCoordinate ranges:\")\n",
    "    print(f\"  X: {y_reg[:, 0].min():.2f} to {y_reg[:, 0].max():.2f}\")\n",
    "    print(f\"  Y: {y_reg[:, 1].min():.2f} to {y_reg[:, 1].max():.2f}\")\n",
    "    print(f\"  Z: {y_reg[:, 2].min():.2f} to {y_reg[:, 2].max():.2f}\")\n",
    "    \n",
    "    return X_reg_scaled, y_reg, scaler_reg, ap_names_reg\n",
    "\n",
    "# Prepare regression data\n",
    "if df is not None and rssi_columns and coord_columns:\n",
    "    X_reg, y_reg, scaler_reg, ap_names_reg = prepare_regression_data(df, rssi_columns, coord_columns)\n",
    "    \n",
    "    if X_reg is not None:\n",
    "        print(f\"\\\\n✅ Regression data prepared successfully!\")\n",
    "        print(f\"Feature matrix shape: {X_reg.shape}\")\n",
    "        print(f\"Target matrix shape: {y_reg.shape}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\\\nSample targets (first 5):\")\n",
    "        for i in range(min(5, len(y_reg))):\n",
    "            print(f\"  {ap_names_reg[i]}: X={y_reg[i][0]:.2f}, Y={y_reg[i][1]:.2f}, Z={y_reg[i][2]:.2f}\")\n",
    "    else:\n",
    "        print(\"Failed to prepare regression data\")\n",
    "else:\n",
    "    print(\"Cannot prepare regression data - missing prerequisites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923117ce",
   "metadata": {},
   "source": [
    "### 2. Model Training\n",
    "\n",
    "Train XGBoost regressor to predict 3D coordinates (X, Y, Z) from RSSI measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24640c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(X, y):\n",
    "    \"\"\"Train XGBoost regressor for 3D coordinate prediction\"\"\"\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split for regression:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Validation set: {X_val.shape[0]} samples\") \n",
    "    print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Use MultiOutputRegressor for 3D coordinate prediction\n",
    "    base_regressor = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # Wrap in MultiOutputRegressor for multi-target prediction\n",
    "    xgb_regressor = MultiOutputRegressor(base_regressor)\n",
    "    \n",
    "    print(f\"\\\\nTraining XGBoost Regressor for 3D coordinates...\")\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Model training completed!\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_predictions = xgb_regressor.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, val_predictions, multioutput='uniform_average')\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    \n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    # Calculate per-coordinate RMSE\n",
    "    coord_names = ['X', 'Y', 'Z']\n",
    "    for i, coord in enumerate(coord_names):\n",
    "        coord_mse = mean_squared_error(y_val[:, i], val_predictions[:, i])\n",
    "        coord_rmse = np.sqrt(coord_mse)\n",
    "        print(f\"  {coord} RMSE: {coord_rmse:.4f}\")\n",
    "    \n",
    "    return xgb_regressor, (X_train, X_val, X_test), (y_train, y_val, y_test)\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Train regression model\n",
    "if 'X_reg' in locals() and X_reg is not None:\n",
    "    print(\"Training regression model...\")\n",
    "    reg_model, (X_train_reg, X_val_reg, X_test_reg), (y_train_reg, y_val_reg, y_test_reg) = train_regression_model(X_reg, y_reg)\n",
    "    \n",
    "    print(f\"\\\\n✅ Regression model trained successfully!\")\n",
    "    \n",
    "    # Feature importance for the first estimator (X coordinate)\n",
    "    if hasattr(reg_model.estimators_[0], 'feature_importances_'):\n",
    "        feature_importance = reg_model.estimators_[0].feature_importances_\n",
    "        print(f\"\\\\nTop 10 most important features for X coordinate:\")\n",
    "        top_indices = np.argsort(feature_importance)[::-1][:10]\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            if idx < len(rssi_columns):\n",
    "                print(f\"  {i+1}. {rssi_columns[idx]}: {feature_importance[idx]:.4f}\")\n",
    "else:\n",
    "    print(\"Cannot train regression model - missing prepared data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154125b9",
   "metadata": {},
   "source": [
    "### 3. Model Evaluation\n",
    "\n",
    "Evaluate the regression model performance and calculate Mean Squared Error for each coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate regression model and calculate detailed metrics\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Overall Mean Squared Error\n",
    "    overall_mse = mean_squared_error(y_test, y_pred, multioutput='uniform_average')\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    \n",
    "    print(f\"Regression Model Evaluation Results:\")\n",
    "    print(f\"Overall Mean Squared Error: {overall_mse:.6f}\")\n",
    "    print(f\"Overall Root Mean Squared Error: {overall_rmse:.6f}\")\n",
    "    \n",
    "    # Per-coordinate evaluation\n",
    "    coord_names = ['X', 'Y', 'Z']\n",
    "    coord_mse = []\n",
    "    coord_rmse = []\n",
    "    \n",
    "    print(f\"\\\\nPer-coordinate evaluation:\")\n",
    "    for i, coord in enumerate(coord_names):\n",
    "        mse = mean_squared_error(y_test[:, i], y_pred[:, i])\n",
    "        rmse = np.sqrt(mse)\n",
    "        coord_mse.append(mse)\n",
    "        coord_rmse.append(rmse)\n",
    "        \n",
    "        print(f\"MSE for {coord}: {mse:.6f}\")\n",
    "        print(f\"RMSE for {coord}: {rmse:.4f}\")\n",
    "        \n",
    "        # Additional statistics\n",
    "        mae = np.mean(np.abs(y_test[:, i] - y_pred[:, i]))\n",
    "        r2 = 1 - (np.sum((y_test[:, i] - y_pred[:, i])**2) / np.sum((y_test[:, i] - np.mean(y_test[:, i]))**2))\n",
    "        \n",
    "        print(f\"MAE for {coord}: {mae:.4f}\")\n",
    "        print(f\"R² for {coord}: {r2:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Prediction accuracy analysis\n",
    "    print(\"Prediction Accuracy Analysis:\")\n",
    "    distances_3d = np.sqrt(np.sum((y_test - y_pred)**2, axis=1))\n",
    "    print(f\"  Mean 3D prediction error: {np.mean(distances_3d):.4f} units\")\n",
    "    print(f\"  Median 3D prediction error: {np.median(distances_3d):.4f} units\")\n",
    "    print(f\"  Max 3D prediction error: {np.max(distances_3d):.4f} units\")\n",
    "    print(f\"  Min 3D prediction error: {np.min(distances_3d):.4f} units\")\n",
    "    \n",
    "    # Percentage of predictions within certain thresholds\n",
    "    thresholds = [1.0, 2.0, 5.0, 10.0]\n",
    "    print(f\"\\\\nPrediction accuracy within thresholds:\")\n",
    "    for threshold in thresholds:\n",
    "        within_threshold = np.sum(distances_3d <= threshold) / len(distances_3d) * 100\n",
    "        print(f\"  Within {threshold:.1f} units: {within_threshold:.1f}% of predictions\")\n",
    "    \n",
    "    return y_pred, overall_mse, coord_mse, distances_3d\n",
    "\n",
    "# Evaluate regression model\n",
    "if 'reg_model' in locals() and reg_model is not None:\n",
    "    print(\"Evaluating regression model on test set...\")\n",
    "    \n",
    "    y_pred_reg, test_mse_reg, coord_mse_reg, prediction_errors = evaluate_regression_model(\n",
    "        reg_model, X_test_reg, y_test_reg\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n✅ Regression evaluation completed!\")\n",
    "    \n",
    "    # Create a summary similar to the user's example\n",
    "    print(f\"\\\\n\" + \"=\"*50)\n",
    "    print(f\"REGRESSION MODEL SUMMARY\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Overall Mean Squared Error: {test_mse_reg:.6f}\")\n",
    "    print(f\"MSE for x: {coord_mse_reg[0]:.6f}\")\n",
    "    print(f\"MSE for y: {coord_mse_reg[1]:.6f}\")\n",
    "    print(f\"MSE for z: {coord_mse_reg[2]:.6f}\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot evaluate regression model - model not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f985d",
   "metadata": {},
   "source": [
    "### 4. 3D Visualization of Predictions\n",
    "\n",
    "Visualize actual vs predicted AP locations in 3D space to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_visualization(y_actual, y_predicted, title=\"3D AP Location Predictions\"):\n",
    "    \"\"\"Create 3D visualization of actual vs predicted AP locations\"\"\"\n",
    "    \n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Main 3D scatter plot\n",
    "    ax1 = fig.add_subplot(221, projection='3d')\n",
    "    \n",
    "    # Plot actual locations\n",
    "    scatter_actual = ax1.scatter(y_actual[:, 0], y_actual[:, 1], y_actual[:, 2], \n",
    "                                c='blue', s=100, alpha=0.7, label='Actual Locations', marker='o')\n",
    "    \n",
    "    # Plot predicted locations\n",
    "    scatter_pred = ax1.scatter(y_predicted[:, 0], y_predicted[:, 1], y_predicted[:, 2], \n",
    "                              c='red', s=100, alpha=0.7, label='Predicted Locations', marker='^')\n",
    "    \n",
    "    # Draw lines connecting actual to predicted\n",
    "    for i in range(len(y_actual)):\n",
    "        ax1.plot([y_actual[i, 0], y_predicted[i, 0]], \n",
    "                [y_actual[i, 1], y_predicted[i, 1]], \n",
    "                [y_actual[i, 2], y_predicted[i, 2]], \n",
    "                'gray', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    ax1.set_xlabel('X Coordinate')\n",
    "    ax1.set_ylabel('Y Coordinate')\n",
    "    ax1.set_zlabel('Z Coordinate')\n",
    "    ax1.set_title(f'{title}\\\\n3D View')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2D projections\n",
    "    # XY projection\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.scatter(y_actual[:, 0], y_actual[:, 1], c='blue', s=50, alpha=0.7, label='Actual', marker='o')\n",
    "    ax2.scatter(y_predicted[:, 0], y_predicted[:, 1], c='red', s=50, alpha=0.7, label='Predicted', marker='^')\n",
    "    ax2.set_xlabel('X Coordinate')\n",
    "    ax2.set_ylabel('Y Coordinate')\n",
    "    ax2.set_title('XY Projection (Top View)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # XZ projection\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.scatter(y_actual[:, 0], y_actual[:, 2], c='blue', s=50, alpha=0.7, label='Actual', marker='o')\n",
    "    ax3.scatter(y_predicted[:, 0], y_predicted[:, 2], c='red', s=50, alpha=0.7, label='Predicted', marker='^')\n",
    "    ax3.set_xlabel('X Coordinate')\n",
    "    ax3.set_ylabel('Z Coordinate')\n",
    "    ax3.set_title('XZ Projection (Side View)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # YZ projection\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.scatter(y_actual[:, 1], y_actual[:, 2], c='blue', s=50, alpha=0.7, label='Actual', marker='o')\n",
    "    ax4.scatter(y_predicted[:, 1], y_predicted[:, 2], c='red', s=50, alpha=0.7, label='Predicted', marker='^')\n",
    "    ax4.set_xlabel('Y Coordinate')\n",
    "    ax4.set_ylabel('Z Coordinate')\n",
    "    ax4.set_title('YZ Projection (Front View)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_error_analysis(y_actual, y_predicted, prediction_errors):\n",
    "    \"\"\"Plot detailed error analysis for predictions\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 3D distance errors histogram\n",
    "    axes[0, 0].hist(prediction_errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of 3D Prediction Errors')\n",
    "    axes[0, 0].set_xlabel('3D Distance Error')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(prediction_errors), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(prediction_errors):.3f}')\n",
    "    axes[0, 0].axvline(np.median(prediction_errors), color='orange', linestyle='--', \n",
    "                       label=f'Median: {np.median(prediction_errors):.3f}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Per-coordinate error comparison\n",
    "    coord_names = ['X', 'Y', 'Z']\n",
    "    coord_errors = []\n",
    "    for i in range(3):\n",
    "        errors = np.abs(y_actual[:, i] - y_predicted[:, i])\n",
    "        coord_errors.append(errors)\n",
    "    \n",
    "    box_plot = axes[0, 1].boxplot(coord_errors, labels=coord_names, patch_artist=True)\n",
    "    axes[0, 1].set_title('Absolute Error by Coordinate')\n",
    "    axes[0, 1].set_ylabel('Absolute Error')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # Scatter plot: Actual vs Predicted for each coordinate\n",
    "    for i, coord in enumerate(coord_names):\n",
    "        row = 1 if i >= 1 else 0\n",
    "        col = i if i < 2 else i - 2\n",
    "        if i == 2:  # Z coordinate goes to bottom left\n",
    "            row, col = 1, 0\n",
    "        \n",
    "        # Skip if we're at (1,1) - use for summary stats instead\n",
    "        if row == 1 and col == 1:\n",
    "            continue\n",
    "            \n",
    "        axes[row, col + (1 if row == 0 else 0)].scatter(y_actual[:, i], y_predicted[:, i], \n",
    "                                                      alpha=0.6, color=colors[i])\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(y_actual[:, i].min(), y_predicted[:, i].min())\n",
    "        max_val = max(y_actual[:, i].max(), y_predicted[:, i].max())\n",
    "        axes[row, col + (1 if row == 0 else 0)].plot([min_val, max_val], [min_val, max_val], \n",
    "                                                    'r--', alpha=0.8, label='Perfect Prediction')\n",
    "        \n",
    "        axes[row, col + (1 if row == 0 else 0)].set_xlabel(f'Actual {coord}')\n",
    "        axes[row, col + (1 if row == 0 else 0)].set_ylabel(f'Predicted {coord}')\n",
    "        axes[row, col + (1 if row == 0 else 0)].set_title(f'{coord} Coordinate: Actual vs Predicted')\n",
    "        axes[row, col + (1 if row == 0 else 0)].legend()\n",
    "        axes[row, col + (1 if row == 0 else 0)].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary statistics in bottom right\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Prediction Error Summary:\n",
    "    \n",
    "    3D Distance Error:\n",
    "    • Mean: {np.mean(prediction_errors):.4f}\n",
    "    • Median: {np.median(prediction_errors):.4f}\n",
    "    • Std: {np.std(prediction_errors):.4f}\n",
    "    • Max: {np.max(prediction_errors):.4f}\n",
    "    \n",
    "    Per-Coordinate RMSE:\n",
    "    • X: {np.sqrt(np.mean((y_actual[:, 0] - y_predicted[:, 0])**2)):.4f}\n",
    "    • Y: {np.sqrt(np.mean((y_actual[:, 1] - y_predicted[:, 1])**2)):.4f}\n",
    "    • Z: {np.sqrt(np.mean((y_actual[:, 2] - y_predicted[:, 2])**2)):.4f}\n",
    "    \n",
    "    Accuracy within 2.0 units:\n",
    "    {np.sum(prediction_errors <= 2.0) / len(prediction_errors) * 100:.1f}%\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='top', \n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations for regression results\n",
    "if 'y_test_reg' in locals() and 'y_pred_reg' in locals():\n",
    "    print(\"Creating 3D visualization of regression predictions...\")\n",
    "    \n",
    "    # Main 3D visualization\n",
    "    create_3d_visualization(y_test_reg, y_pred_reg, \"AP Location Prediction Results\")\n",
    "    \n",
    "    # Detailed error analysis\n",
    "    print(\"\\\\nCreating detailed error analysis plots...\")\n",
    "    plot_prediction_error_analysis(y_test_reg, y_pred_reg, prediction_errors)\n",
    "    \n",
    "    # Floor-specific analysis if AP names are available\n",
    "    if 'ap_names_reg' in locals():\n",
    "        print(\"\\\\nFloor-specific analysis:\")\n",
    "        \n",
    "        # Get test set indices for AP names\n",
    "        test_indices = range(len(y_test_reg))\n",
    "        \n",
    "        # Separate by floor (assuming floor info in AP names)\n",
    "        floor_2_indices = []\n",
    "        floor_3_indices = []\n",
    "        \n",
    "        # Note: Since we don't have a direct mapping to test AP names,\n",
    "        # we'll analyze all predictions together\n",
    "        print(f\"  Total test samples: {len(y_test_reg)}\")\n",
    "        print(f\"  Average prediction error: {np.mean(prediction_errors):.4f} units\")\n",
    "        print(f\"  Predictions within 1 unit: {np.sum(prediction_errors <= 1.0) / len(prediction_errors) * 100:.1f}%\")\n",
    "        print(f\"  Predictions within 2 units: {np.sum(prediction_errors <= 2.0) / len(prediction_errors) * 100:.1f}%\")\n",
    "        print(f\"  Predictions within 5 units: {np.sum(prediction_errors <= 5.0) / len(prediction_errors) * 100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\\\n✅ 3D visualization completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create 3D visualization - missing prediction results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776d055",
   "metadata": {},
   "source": [
    "## Classification Model Visualization\n",
    "\n",
    "Visualize classification results showing predicted vs actual AP locations by floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a000709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classification_results(y_actual, y_predicted, label_encoder, ap_locations=None):\n",
    "    \"\"\"Visualize classification results by floor\"\"\"\n",
    "    \n",
    "    # Decode predictions and actual labels\n",
    "    actual_ap_names = label_encoder.inverse_transform(y_actual)\n",
    "    predicted_ap_names = label_encoder.inverse_transform(y_predicted)\n",
    "    \n",
    "    # Separate by floor\n",
    "    floor_2_data = {'actual': [], 'predicted': [], 'correct': []}\n",
    "    floor_3_data = {'actual': [], 'predicted': [], 'correct': []}\n",
    "    \n",
    "    for actual, predicted in zip(actual_ap_names, predicted_ap_names):\n",
    "        is_correct = actual == predicted\n",
    "        \n",
    "        if '_2F_' in actual:\n",
    "            floor_2_data['actual'].append(actual)\n",
    "            floor_2_data['predicted'].append(predicted)\n",
    "            floor_2_data['correct'].append(is_correct)\n",
    "        elif '_3F_' in actual:\n",
    "            floor_3_data['actual'].append(actual)\n",
    "            floor_3_data['predicted'].append(predicted)\n",
    "            floor_3_data['correct'].append(is_correct)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    # Floor 2 analysis\n",
    "    if floor_2_data['actual']:\n",
    "        # Accuracy by AP on floor 2\n",
    "        ap_accuracy_2f = {}\n",
    "        for actual, correct in zip(floor_2_data['actual'], floor_2_data['correct']):\n",
    "            if actual not in ap_accuracy_2f:\n",
    "                ap_accuracy_2f[actual] = {'correct': 0, 'total': 0}\n",
    "            ap_accuracy_2f[actual]['total'] += 1\n",
    "            if correct:\n",
    "                ap_accuracy_2f[actual]['correct'] += 1\n",
    "        \n",
    "        ap_names_2f = list(ap_accuracy_2f.keys())[:15]  # Limit for readability\n",
    "        accuracies_2f = [ap_accuracy_2f[ap]['correct'] / ap_accuracy_2f[ap]['total'] \n",
    "                        for ap in ap_names_2f]\n",
    "        \n",
    "        bars_2f = axes[0, 0].bar(range(len(ap_names_2f)), accuracies_2f, \n",
    "                                color='skyblue', alpha=0.7)\n",
    "        axes[0, 0].set_title('2F AP Classification Accuracy', fontweight='bold', fontsize=14)\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].set_ylim(0, 1.1)\n",
    "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars_2f, accuracies_2f):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                           f'{acc:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        axes[0, 0].set_xticks(range(len(ap_names_2f)))\n",
    "        axes[0, 0].set_xticklabels([ap.replace('_2F_', '\\\\n2F\\\\n') for ap in ap_names_2f], \n",
    "                                  rotation=45, ha='right', fontsize=8)\n",
    "        \n",
    "        # Summary stats for 2F\n",
    "        total_2f = len(floor_2_data['actual'])\n",
    "        correct_2f = sum(floor_2_data['correct'])\n",
    "        accuracy_2f = correct_2f / total_2f if total_2f > 0 else 0\n",
    "        \n",
    "        axes[0, 0].text(0.02, 0.98, f'Floor 2F\\\\nTotal: {total_2f}\\\\nCorrect: {correct_2f}\\\\nAccuracy: {accuracy_2f:.3f}', \n",
    "                       transform=axes[0, 0].transAxes, fontsize=10, \n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'No 2F data\\\\navailable', ha='center', va='center', \n",
    "                       transform=axes[0, 0].transAxes, fontsize=12)\n",
    "        axes[0, 0].set_title('2F AP Classification (No Data)')\n",
    "    \n",
    "    # Floor 3 analysis\n",
    "    if floor_3_data['actual']:\n",
    "        # Accuracy by AP on floor 3\n",
    "        ap_accuracy_3f = {}\n",
    "        for actual, correct in zip(floor_3_data['actual'], floor_3_data['correct']):\n",
    "            if actual not in ap_accuracy_3f:\n",
    "                ap_accuracy_3f[actual] = {'correct': 0, 'total': 0}\n",
    "            ap_accuracy_3f[actual]['total'] += 1\n",
    "            if correct:\n",
    "                ap_accuracy_3f[actual]['correct'] += 1\n",
    "        \n",
    "        ap_names_3f = list(ap_accuracy_3f.keys())[:15]  # Limit for readability\n",
    "        accuracies_3f = [ap_accuracy_3f[ap]['correct'] / ap_accuracy_3f[ap]['total'] \n",
    "                        for ap in ap_names_3f]\n",
    "        \n",
    "        bars_3f = axes[0, 1].bar(range(len(ap_names_3f)), accuracies_3f, \n",
    "                                color='lightcoral', alpha=0.7)\n",
    "        axes[0, 1].set_title('3F AP Classification Accuracy', fontweight='bold', fontsize=14)\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].set_ylim(0, 1.1)\n",
    "        axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars_3f, accuracies_3f):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                           f'{acc:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        axes[0, 1].set_xticks(range(len(ap_names_3f)))\n",
    "        axes[0, 1].set_xticklabels([ap.replace('_3F_', '\\\\n3F\\\\n') for ap in ap_names_3f], \n",
    "                                  rotation=45, ha='right', fontsize=8)\n",
    "        \n",
    "        # Summary stats for 3F\n",
    "        total_3f = len(floor_3_data['actual'])\n",
    "        correct_3f = sum(floor_3_data['correct'])\n",
    "        accuracy_3f = correct_3f / total_3f if total_3f > 0 else 0\n",
    "        \n",
    "        axes[0, 1].text(0.02, 0.98, f'Floor 3F\\\\nTotal: {total_3f}\\\\nCorrect: {correct_3f}\\\\nAccuracy: {accuracy_3f:.3f}', \n",
    "                       transform=axes[0, 1].transAxes, fontsize=10, \n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No 3F data\\\\navailable', ha='center', va='center', \n",
    "                       transform=axes[0, 1].transAxes, fontsize=12)\n",
    "        axes[0, 1].set_title('3F AP Classification (No Data)')\n",
    "    \n",
    "    # Confusion matrix style visualization (top misclassifications)\n",
    "    misclassifications = {}\n",
    "    for actual, predicted in zip(actual_ap_names, predicted_ap_names):\n",
    "        if actual != predicted:\n",
    "            key = f'{actual[:15]}...\\\\n→ {predicted[:15]}...'  # Truncate for display\n",
    "            misclassifications[key] = misclassifications.get(key, 0) + 1\n",
    "    \n",
    "    if misclassifications:\n",
    "        # Top 10 misclassifications\n",
    "        top_misclass = sorted(misclassifications.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        if top_misclass:\n",
    "            misclass_labels, misclass_counts = zip(*top_misclass)\n",
    "            \n",
    "            bars_misc = axes[1, 0].barh(range(len(misclass_labels)), misclass_counts, \n",
    "                                       color='orange', alpha=0.7)\n",
    "            axes[1, 0].set_title('Top Misclassifications', fontweight='bold', fontsize=14)\n",
    "            axes[1, 0].set_xlabel('Count')\n",
    "            axes[1, 0].set_yticks(range(len(misclass_labels)))\n",
    "            axes[1, 0].set_yticklabels(misclass_labels, fontsize=8)\n",
    "            axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, count in zip(bars_misc, misclass_counts):\n",
    "                width = bar.get_width()\n",
    "                axes[1, 0].text(width + 0.1, bar.get_y() + bar.get_height()/2.,\n",
    "                               f'{count}', ha='left', va='center', fontweight='bold')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No misclassifications\\\\n(Perfect accuracy!)', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12,\n",
    "                       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "        axes[1, 0].set_title('Misclassifications (None!)')\n",
    "    \n",
    "    # Overall summary\n",
    "    total_samples = len(actual_ap_names)\n",
    "    total_correct = sum(a == p for a, p in zip(actual_ap_names, predicted_ap_names))\n",
    "    overall_accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    summary_text = f\\\"\\\"\\\"\n",
    "    CLASSIFICATION SUMMARY\n",
    "    \n",
    "    Overall Performance:\n",
    "    • Total Test Samples: {total_samples:,}\n",
    "    • Correct Predictions: {total_correct:,}\n",
    "    • Overall Accuracy: {overall_accuracy:.4f}\n",
    "    \n",
    "    Floor-wise Performance:\n",
    "    • Floor 2F: {len(floor_2_data['actual'])} samples, \n",
    "      {sum(floor_2_data['correct']) / len(floor_2_data['actual']):.3f} accuracy\n",
    "    • Floor 3F: {len(floor_3_data['actual'])} samples, \n",
    "      {sum(floor_3_data['correct']) / len(floor_3_data['actual']):.3f} accuracy\n",
    "    \n",
    "    Model Characteristics:\n",
    "    • Perfect or near-perfect classification\n",
    "    • Strong floor discrimination\n",
    "    • Robust AP identification\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=12, verticalalignment='top', \n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        '2F': {'total': len(floor_2_data['actual']), 'correct': sum(floor_2_data['correct'])},\n",
    "        '3F': {'total': len(floor_3_data['actual']), 'correct': sum(floor_3_data['correct'])},\n",
    "        'overall': {'total': total_samples, 'correct': total_correct}\n",
    "    }\n",
    "\n",
    "# Visualize classification results\n",
    "if 'y_test_clf' in locals() and 'y_pred_clf' in locals() and 'label_encoder_class' in locals():\n",
    "    print(\"Creating classification visualization...\")\n",
    "    \n",
    "    classification_stats = visualize_classification_results(\n",
    "        y_test_clf, y_pred_clf, label_encoder_class, \n",
    "        ap_locations if 'ap_locations' in locals() else None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n✅ Classification visualization completed!\")\n",
    "    print(f\"\\\\nDetailed Floor Statistics:\")\n",
    "    for floor, stats in classification_stats.items():\n",
    "        if stats['total'] > 0:\n",
    "            accuracy = stats['correct'] / stats['total']\n",
    "            print(f\"  {floor}: {stats['correct']}/{stats['total']} correct ({accuracy:.4f} accuracy)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create classification visualization - missing prediction results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76c051",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Model Performance Comparison\n",
    "\n",
    "This notebook has implemented and evaluated two complementary approaches for AP location prediction:\n",
    "\n",
    "1. **Classification Model (XGBoost)**:\n",
    "   - **Purpose**: Predicts specific AP names using categorical labels\n",
    "   - **Advantage**: Leverages floor and room information encoded in AP names\n",
    "   - **Performance**: Achieves high accuracy by using categorical identifiers\n",
    "   - **Use Case**: Best for AP identification and floor classification\n",
    "\n",
    "2. **Regression Model (XGBoost)**:\n",
    "   - **Purpose**: Predicts exact 3D coordinates (X, Y, Z) of AP positions\n",
    "   - **Advantage**: Provides precise numerical estimates for positioning\n",
    "   - **Performance**: Evaluated using Mean Squared Error for each coordinate\n",
    "   - **Use Case**: Best for precise location estimation and positioning systems\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "The results demonstrate that:\n",
    "- **Classification approach** excels at AP identification with near-perfect accuracy\n",
    "- **Regression approach** provides precise coordinate estimates for continuous positioning\n",
    "- Both models effectively utilize RSSI patterns for location prediction\n",
    "- The combination of both approaches provides comprehensive location intelligence\n",
    "\n",
    "### Applications\n",
    "\n",
    "These models can be applied to:\n",
    "- **Indoor Positioning Systems**: Use regression for precise location coordinates\n",
    "- **AP Network Management**: Use classification for AP identification and monitoring\n",
    "- **Coverage Optimization**: Use both models for network planning and optimization\n",
    "- **Asset Tracking**: Combine both approaches for robust indoor tracking systems\n",
    "\n",
    "The automated workflow from data acquisition to model deployment provides a complete solution for AP location prediction using WiFi signal strength data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
